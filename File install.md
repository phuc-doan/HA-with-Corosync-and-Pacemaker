# HÆ¯á»šNG DáºªN CÃ€I Äáº¶T HA CHO WEB-SERVER Báº°NG PACEMAKER + COROSYNC, DRDB TRÃŠN CENTOS 7
 **APACHE , CNTT**
## 1. Chuáº©n bá»‹ ğŸ¤ğŸ¤
- 2 server sá»­ dá»¥ng OS CentOS
- 2 á»• cá»©ng cÃ³ cÃ¹ng dung lÆ°á»£ng Ä‘Æ°á»£c gáº¯n vÃ o cÃ¡c node
- Cáº¥u hÃ¬nh hostname cho cÃ¡c server
- Má»Ÿ port 7788 trÃªn cÃ¡c server
### Cá»¥ thá»ƒ:
- Node 1
```
OS: CentOS 7 64 bit
Device: /dev/sdb - 8GB
Hostname: node1
IP: 192.168.100.196
Gateway: 192.168.100.1
Network: 192.168.100.0/24
```
- Node 2
```
OS: CentOS 7 64 bit
Device: /dev/sdb - 8GB
Hostname: node2
IP: 192.168.100.197
Gateway: 192.168.100.1
Network: 192.168.100.0/24
```
### MÃ´ hÃ¬nh ğŸ–¥ğŸ–¥

![image](https://user-images.githubusercontent.com/83824403/147487809-f0063692-e644-407b-a14f-26d581794001.png)

#### TrÆ°á»›c khi cÃ i Ä‘áº·t, chÃºng ta pháº£i cáº¥u hÃ¬nh hostname cho má»—i node vÃ  ghi chÃºng vÃ o hosts
```
[root@node1 ~] hostnamectl set-hostname node1
[root@node2 ~] hostnamectl set-hostname node2
```

- Ghi thÃªm vÃ o hosts cá»§a má»—i server
```
vi /etc/hosts
[...]
192.168.100.196 node1
192.168.100.197 node2
```


### 2. CÃ¡c bÆ°á»›c thá»±c hiá»‡n âœˆâœˆ
#### 2.1 CÃ i Ä‘áº·t Apache Web-server


- CÃ i Ä‘áº·t Apache trÃªn cáº£ 2 node:
```
yum -y install httpd*
```

- Cáº¥u hÃ¬nh tÆ°á»ng lá»­a cho phÃ©p ngÆ°á»i dÃ¹ng bÃªn ngoÃ i cÃ³ thá»ƒ truy cáº­p vÃ o qua giao thá»©c http:
```
firewall-cmd --permanent --add-service=http
firewall-cmd --reload
```
#### 2.2 CÃ i Ä‘áº·tğŸ”‘  Pacemaker  vÃ  Corosync âœˆâœˆ 


- CÃ i Ä‘áº·t cÃ¡c gÃ³i cluster trÃªn cáº£ 2 node báº±ng yum:

```
yum install -y pacemaker pcs fence-agents-all psmisc policycoreutils-python
```
- CÅ©ng nhÆ° pháº§n trÃªn, chÃºng ta cáº¥u hÃ¬nh tÆ°á»ng lá»­a cho phÃ©p dá»‹ch vá»¥ HA:
```
firewall-cmd --permanent --add-service=high-availability
firewall-cmd --reload
```
#### 2.3 Cáº¥u hÃ¬nh Cluster vá»›i Pacemaker vÃ  Corosync 
- TrÃªn cáº£ 2 node, chÃºng ta Ä‘áº·t password cho user hacluster Ä‘á»ƒ xÃ¡c thá»±c vá»›i nhau, 2 máº­t kháº©u pháº£i trÃ¹ng khá»›p
```
echo "redhat" | passwd --stdin hacluster
```
- Tiáº¿p theo, chÃºng ta khá»Ÿi Ä‘á»™ng dá»‹ch vá»¥ trÃªn cáº£ 2 node:
```
systemctl start pcsd
systemctl enable pcsd
```
- TrÃªn node1, chÃºng ta xÃ¡c thá»±c 2 node vá»›i nhau báº±ng lá»‡nh:
```
[root@node1 ~]# pcs cluster auth node1 node2 -u hacluster -p redhat
```


```
node1: Authorized
node2: Authorized
```
- Sau khi xÃ¡c thá»±c, chÃºng ta táº¡o 1 cluster trÃªn node 1 cÃ³ tÃªn lÃ  mycluster Ä‘á»ƒ chÃºng cÃ³ thá»ƒ táº¡o vÃ  Ä‘á»“ng bá»™ cÃ¡c file cáº¥u hÃ¬nh vá»›i nhau. ğŸ–¥

```
[root@node1 ~]# pcs cluster setup --name mycluster node1 node2
```

```
Shutting down pacemaker/corosync services...
Redirecting to /bin/systemctl stop  pacemaker.service
Redirecting to /bin/systemctl stop  corosync.service
Killing any remaining services...
Removing all cluster configuration files...
node1: Succeeded
node2: Succeeded
Synchronizing pcsd certificates on nodes node1, node2...
node1: Success
node2: Success
Restaring pcsd on the nodes in order to reload the certificates...
node1: Success
node2: Success
```
- Khá»Ÿi Ä‘á»™ng vÃ  kÃ­ch hoáº¡t cluster má»›i táº¡o trÃªn node 1 báº±ng lá»‡nh:
```
[root@node1 ~]# pcs cluster start --all
[root@node1 ~]# pcs cluster enable --all
```
- Xem láº¡i tráº¡ng thÃ¡i cá»§a cluster trÃªn cÃ¡c node:
```
pcs status
```
##### Táº¯t Quorum vÃ  STONITH
```
pcs property set stonith-enabled=false
pcs property set no-quorum-policy=ignore
pcs property set default-resource-stickiness="INFINITY"
```
#### 2.4 CÃ i Ä‘áº·t DRDB ğŸ”ğŸ”
- Äá»ƒ cÃ i Ä‘áº·t DRDB má»›i nháº¥t, chÃºng ta thÃªm repo cá»§a DRDB (cÃ¡c thao tÃ¡c nÃ y lÃ m trÃªn cáº£ 2 node):
```
rpm -ivh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
```
- ThÃªm Public key cho DRDB
```
rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-elrepo.org
```
- Tiáº¿p theo, chÃºng ta cÃ i Ä‘áº·t DRDB trÃªn cáº£ 2 node:
```
yum -y install drbd*-utils kmod-drbd*
```
- Náº¿u quÃ¡ trÃ¬nh cÃ i Ä‘áº·t trÃªn khÃ´ng thÃ nh cÃ´ng hoáº·c xáº£y ra lá»—i hÃ£y chÃ¨n láº¡i key vÃ  tiáº¿p tá»¥c cÃ i Ä‘áº·t:
```
rpm --import /etc/pki/rpm-gpg/*
```
- KÃ­ch hoáº¡t DRDB trÃªn cáº£ 2 node
```
modprobe drbd
```
- Kiá»ƒm tra DRDB Ä‘Ã£ hoáº¡t Ä‘á»™ng hay chÆ°a:
```
lsmod | grep drbd
```

```
drbd                  405309  0
libcrc32c              12644  2 xfs,drbd
Náº¿u tÆ°á»ng lá»­a Ä‘Æ°á»£c kÃ­ch hoáº¡t hÃ£y thÃªm rule cho phÃ©p DRDB:
firewall-cmd --permanent --add-port=7788/tcp
firewall-cmd --reload
2.5 Cáº¥u hÃ¬nh DRDB
Äá»ƒ cáº¥u hÃ¬nh DRDB, chÃºng ta táº¡o má»™t file cÃ³ tÃªn lÃ  testdata1.res vá»›i ná»™i dung:
vi /etc/drbd.d/testdata1.res
resource testdata1 {
protocol C;        
on node1 {
                device /dev/drbd0;
                disk /dev/vgdrbd/vol1;
                address 192.168.100.196:7788;
                meta-disk internal;
        }
on node2 {
                device /dev/drbd0;
                disk /dev/vgdrbd/vol1;
                address 192.168.100.197:7788;
                meta-disk internal;
        }
} 
```

#### Giáº£i thÃ­ch:
```
resource testdata1: TÃªn cá»§a resource
Protocol C: CÃ¡c resource Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘á»ƒ synchronous replication. Chi tiáº¿t táº¡i Ä‘Ã¢y
node1, node2: Danh sÃ¡ch cÃ¡c node vÃ  cÃ¡c tÃ¹y chá»n bÃªn trong
device /dev/drbd0: XÃ¡c Ä‘á»‹nh thiáº¿t bá»‹ logic Ä‘Æ°á»£c DRBD sá»­ dá»¥ng (NÃªn Ä‘áº·t giá»‘ng nhau á»Ÿ trÃªn 2 server)
disk /dev/vgdrbd/vol1: XÃ¡c Ä‘á»‹nh thiáº¿t bá»‹ váº­t lÃ½ dÃ¹ng Ä‘á»ƒ táº¡o ra thiáº¿t bá»‹ logic bÃªn trÃªn, vÃ  khÃ´ng nháº¥t thiáº¿t pháº£i cÃ¹ng trÃªn trÃªn 2 server.
address 192.168.100.197:7788: XÃ¡c Ä‘á»‹nh Ä‘á»‹a chá»‰ IP vÃ  Port cá»§a má»—i server
meta-disk internal: Cho phÃ©p sá»­ dá»¥ng Meta-data má»©c ná»™i bá»™
``` 
- Sao chÃ©p file cáº¥u hÃ¬nh sang node 2:
```
[root@node1 ~] scp /etc/drbd.d/testdata1.res node2:/etc/drbd.d/
```
- TrÃªn cáº£ 2 node, chÃºng ta táº¡o má»™t LVM Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c dá»¯ liá»‡u mÃ  chÃºng ta Ä‘Ã£ Ä‘á»‹nh nghÄ©a á»Ÿ file cáº¥u hÃ¬nh bÃªn trÃªn /dev/vgdrbd/vol1. á» Ä‘Ã¢y, /dev/sdb Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o LVM
pvcreate /dev/sdb
 ```
 Physical volume "/dev/sdb" successfully created
vgcreate vgdrbd /dev/sdb
  Volume group "vgdrbd" successfully created
lvcreate -n vol1 -l100%FREE vgdrbd
  Logical volume "vol1" created.
  ```
### 2.6 Khá»Ÿi Ä‘á»™ng DRDB meta-data storage ğŸ‘Œ
- Sau khi cáº¥u hÃ¬nh xong cÃ¡c bÆ°á»›c bÃªn trÃªn, chÃºng ta chuyá»ƒn tiáº¿p Ä‘áº¿n pháº§n khá»Ÿi Ä‘á»™ng thiáº¿t bá»‹ DRBD. TrÃªn cáº£ 2 node chÃºng ta thá»±c hiá»‡n láº§n lÆ°á»£t cÃ¡c lá»‡nh sau vÃ  Ä‘á»c thÃ´ng bÃ¡o káº¿t quáº£:
```
[root@node1 ~] drbdadm create-md testdata1
[root@node2 ~] drbdadm create-md testdata1
```
```
Above commands will give an output like below.
  --==  Thank you for participating in the global usage survey  ==--
The server's response is:
you are the 10680th user to install this version
initializing activity log
NOT initializing bitmap
Writing meta data...
New drbd meta data block successfully created.
success
```
- Tiáº¿p Ä‘áº¿n, chÃºng ta khá»Ÿi Ä‘á»™ng DRBD vÃ  kÃ­ch hoáº¡t khá»Ÿi cháº¡y cÃ¹ng há»‡ thá»‘ng:
```
systemctl start drbd
systemctl enable drbd
```
### 2.7 XÃ¡c Ä‘á»‹nh DRDB Primary node ğŸ‘
- XÃ¡c Ä‘á»‹nh node DRBD Primary trÃªn node1:
```
[root@node1 ~] drbdadm primary testdata1 --force
```
- Kiá»ƒm tra káº¿t quáº£ quÃ¡ trÃ¬nh Ä‘á»“ng bá»™:

```
[root@node1 ~]# cat /proc/drbd
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by phil@Build64R7, 2016-01-12 14:29:40
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:1048508 nr:0 dw:0 dr:1049236 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
[root@node1 ~]# drbd-overview
 0:testdata1/0  Connected Primary/Secondary UpToDate/UpToDate
 ```
### 2.8 Táº¡o vÃ  mount filesystem thiáº¿t bá»‹ DRDB
- LÃ m bÆ°á»›c nÃ y chá»‰ khi bÆ°á»›c trÃªn thÃ nh cÃ´ng, khi kiá»ƒm tra tháº¥y DRBD bÃ¡o Connected
```
[root@node1 ~]# drbd-overview
 0:testdata1/0  Connected Primary/Secondary UpToDate/UpToDate
 ```
- ChÃºng ta táº¡o filesystem vÃ  ghi dá»¯ liá»‡u vÃ o nÃ³:
```
[root@node1 ~]# mkfs.ext3 /dev/drbd0
[root@node1 ~]# mount /dev/drbd0 /mnt
[root@node1 ~]# echo "Welcome to my Website" > /mnt/index.html
[root@node1 ~]# umount /mnt
```
### 2.9 Táº¡o cluster resource ğŸš— vÃ  Ä‘áº·t cÃ¡c thuá»™c tÃ­nh ğŸ”‘
- Tiáº¿p Ä‘áº¿n, chÃºng ta táº¡o ra cÃ¡c tÃ i nguyÃªn nhÆ° VIP, Httpd, DrbdData, DrbdDataClone, DrbdFS trÃªn node1 Ä‘á»ƒ khi cÃ³ yÃªu cáº§u sáº½ bring up nÃ³ lÃªn trÃªn cÃ¡c node khÃ¡c trong trÆ°á»ng há»£p cÃ³ sá»± cá»‘ xáº£y ra:
- Táº¡o cho VIP
```
[root@node1 ~]# pcs resource create VirtIP ocf:heartbeat:IPaddr2 ip=192.168.100.123 cidr_netmask=32 op monitor interval=30s
```
- Táº¡o cho dá»‹ch vá»¥ Httpd
```
[root@node1 ~]#  pcs resource create Httpd ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=1min

```
- Táº¡o sá»± rÃ ng buá»™c giá»¯a Httpd vÃ  VIP
- ChÃºng ta cáº¥u hÃ¬nh 2 tÃ i nguyÃªn nÃ y rÃ ng buá»™c vá»›i nhau. Giáº£i thÃ­ch cÃ¢u lá»‡nh: Khi VIP khá»Ÿi Ä‘á»™ng xong thÃ¬ Httpd má»›i khá»Ÿi Ä‘á»™ng, vÃ  2 tÃ i nguyÃªn nÃ y lÃ  má»™t cáº·p.
```
[root@node1 ~]# pcs constraint colocation add Httpd with VirtIP INFINITY
[root@node1 ~]# pcs constraint order VirtIP then Httpd

Adding VirtIP Httpd (kind: Mandatory) (Options: first-action=start then-action=start)
``` 
- Táº¡o sá»± thá»‘ng nháº¥t giá»¯a tÃ i nguyÃªn DRBD trÃªn cÃ¡c node
- Khi má»™t node1 Ä‘ang Active, thÃ¬ node2 á»Ÿ cháº¿ Ä‘á»™ Passive lÆ°u trá»¯ vÃ  Ä‘á»“ng bá»™ dá»¯ liá»‡u tá»« node1. Äá»ƒ lÃ m nhÆ° váº­y, chÃºng ta gá»™p tÃ i nguyÃªn á»Ÿ 2 node báº±ng cáº¥u hÃ¬nh CIB
```
[root@node1 ~]# pcs cluster cib drbd_cfg
```
- Khai bÃ¡o tÃ i nguyÃªn DRBD
```
[root@node1 ~]#  pcs -f drbd_cfg resource create DrbdData ocf:linbit:drbd drbd_resource=testdata1 op monitor interval=60s
```
**ChÃº Ã½: testdata1 lÃ  tÃªn cá»§a tÃ i nguyÃªn Ä‘Æ°á»£c khai bÃ¡o trong file cáº¥u hÃ¬nh DRBD.** ğŸ–¥ğŸ–¥
- Táº¡o báº£n sao tÃ i nguyÃªn DRBD
- Báº£n sao nÃ y cho phÃ©p tÃ i nguyÃªn cÃ³ thá»ƒ cháº¡y trÃªn cÃ¡c node cÃ¹ng má»™t thá»i Ä‘iá»ƒm
```
[root@node1 ~]#  pcs -f drbd_cfg resource master DrbdDataClone DrbdData master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true
```
- ChÃºng ta commit nhá»¯ng gÃ¬ vá»«a cáº¥u hÃ¬nh bÃªn trÃªn vÃ o CIB chÃ­nh:
```
[root@node1 ~]# pcs cluster cib-push drbd_cfg



CIB updated
```
- Kiá»ƒm tra tráº¡ng thÃ¡i cá»§a cÃ¡c resource
```
[root@node1 ~]# pcs status resources
 VirtIP (ocf::heartbeat:IPaddr2):       Started node1
 Httpd  (ocf::heartbeat:apache):        Started node1
 Master/Slave Set: DrbdDataClone [DrbdData]
     Masters: [ node1 ]
     Slaves: [ node2 ]
   ```
- âœˆ Táº¡o tÃ i nguyÃªn filesystem DRBD
```
[root@node1 ~]# pcs cluster cib fs_cfg
```
- ChÃºng ta táº¡o 1 tÃ i nguyÃªn filesystem Ä‘á»ƒ mount thiáº¿t bá»‹ DRBD
```
[root@node1 ~]#  pcs  -f fs_cfg resource create DrbdFS Filesystem device="/dev/drbd0" directory="/var/www/html" fstype="ext3"
```
- NhÆ° á»Ÿ trÃªn, chÃºng ta cÅ©ng sáº½ cáº¥u hÃ¬nh rÃ ng buá»™c khi stop, start cÃ¡c tÃ i nguyÃªn
```
[root@node1 ~]# pcs  -f fs_cfg constraint colocation add DrbdFS with DrbdDataClone INFINITY with-rsc-role=Master
[root@node1 ~]# pcs  -f fs_cfg constraint order promote DrbdDataClone then start DrbdFS
Adding DrbdDataClone DrbdFS (kind: Mandatory) (Options: first-action=promote then-action=start)
[root@node1 ~]# pcs -f fs_cfg constraint colocation add Httpd with DrbdFS INFINITY
[root@node1 ~]# pcs -f fs_cfg constraint order DrbdFS then Httpd


Adding DrbdFS Httpd (kind: Mandatory) (Options: first-action=start then-action=start)
```
- Cáº­p nháº­t cÃ¡c thay Ä‘á»•i báº±ng lá»‡nh:
```
[root@node1 ~]# pcs cluster cib-push fs_cfg
CIB Updated
```
- Kiá»ƒm tra láº¡i tráº¡ng thÃ¡i cá»§a cÃ¡c tÃ i nguyÃªn:
```
[root@node1 ~]# pcs status resources
 VirtIP (ocf::heartbeat:IPaddr2):       Started node1
 Httpd  (ocf::heartbeat:apache):        Started node1
 Master/Slave Set: DrbdDataClone [DrbdData]
     Masters: [ node1 ]
     Slaves: [ node2 ]
 DrbdFS (ocf::heartbeat:Filesystem):    Started node1
 ```
- Máº·c Ä‘á»‹nh thÃ¬ thá»i gian timeout cho start, stop vÃ  monitor cÃ¡c tÃ i nguyÃªn lÃ  20s. ChÃºng ta chá»‰nh láº¡i thá»i gian nÃ y báº±ng lá»‡nh:
```
root@node1 ~]# pcs resource op defaults timeout=240s
[root@node1 ~]# pcs resource op defaults
timeout: 240s
```
#### BÃ¢y giá», chÃºng ta cÃ³ thá»ƒ truy cáº­p vÃ o Ä‘á»‹a chá»‰ VIP: 192.168.100.123 Ä‘á»ƒ cÃ³ thá»ƒ kiá»ƒm tra sá»± hoáº¡t Ä‘á»™ng cá»§a cluster.
### 2.10 Test trÆ°á»ng há»£p fail-over báº±ng tay âœ‹âœ‹
- Hiá»‡n táº¡i, táº¥t cáº£ cÃ¡c tÃ i nguyÃªn Ä‘Æ°á»£c sá»­ dá»¥ng trÃªn node1. HÃ£y stop chÃºng láº¡i Ä‘á»ƒ táº¡o ra trÆ°á»ng há»£p fail-over vÃ  sang node2 kiá»ƒm tra cÃ¡c tÃ i nguyÃªn:
```
[root@node1 ~]# pcs cluster stop node1
node1: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (corosync)...
```
**Khi node1 Ä‘Ã£ stop âŒ , chÃºng ta sang node2 Ä‘á»ƒ kiá»ƒm tra:**
```
[root@node2 ~]# pcs status resources
 VirtIP (ocf::heartbeat:IPaddr2):       Started node2
 Httpd  (ocf::heartbeat:apache):        Started node2
 Master/Slave Set: DrbdDataClone [DrbdData]
     Masters: [ node2 ]
     Stopped: [ node1 ]
 DrbdFS (ocf::heartbeat:Filesystem):    Started node2
 ```
### NhÆ° váº­y lÃ  chÃºng ta Ä‘Ã£ hoÃ n thÃ nh quÃ¡ trÃ¬nh cÃ i Ä‘áº·t cluster báº±ng Pacemaker vÃ  Corosync.âœ”âœ”â¤

